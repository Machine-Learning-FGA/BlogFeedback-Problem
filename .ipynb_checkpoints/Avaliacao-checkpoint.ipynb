{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação dos modelos de regressão criados\n",
    "\n",
    "Em termos gerais um regressor é um modelo de _machine learning_ que prevê uma saída, ou resultado, numérico.\n",
    "\n",
    "## _Root-mean-square error_\n",
    "\n",
    "Essa é a forma mais simples de medir a performance de um modelo de regressão. Essa técnica avalia a diferença de cada valor predito para seu valor real, e calcula a média de forma que se torna imune ao fato que valores preditos podem ser tanto maiores quanto menores que seu valor real.\n",
    "\n",
    "Uma vantagem dessa técnica é que o resultado é na mesma unidade que os próprios valores, mas também se torna uma desvantagem no sentido de que o resultado depende da escala do problema. Se o valor predito ou o real forem numéros muito altos o _RMSE_ vai ser correspondentemente alto. Sendo assim, isso pode se tornar um problema quando se deseja comparar modelos de projetos diferentes.\n",
    "\n",
    "$$ RSME = \\frac{1}{\\sqrt{n}} \\sqrt{\\sum{[y_i - f(x_i)]^2}} $$\n",
    "<center>onde, $x_i$ e $y_i$ são os inézimos <i>targets</i> e características e $f(x)$ representa a aplicação do medelo no vetor de características, que retorna o valor predito.</center>\n",
    "\n",
    "\n",
    "## $R^2$\n",
    "\n",
    "$$ R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} $$\n",
    "\n",
    "\n",
    "$$ SS_{res} = \\sum{(y_i - yhat_i)^2} $$\n",
    "<center>onde, $ (y_i - yhat_i) $ é a distância do ponto real na função para o ponto da predição.</center>\n",
    "\n",
    "$$ SS_{tot} = \\sum{(y_i - y_{avg})^2} $$\n",
    "<center>onde, $y_{avg}$ é a linha da média de todos os pontos.</center>\n",
    "\n",
    "Sendo assim, o método de avaliação $R^2$ compara quão bom é o modelo gerado, baseado na linha média dos valores. Quanto $R^2$ está mais perto de $1$ melhor é o modelo, quanto mais longe e perto de $-1$, pior o modelo.\n",
    "\n",
    "$R^2$ tem um grande problema quando tratamos de funções polinomiais, pois o método $R^2$ ou não descresce ou não muda, dependendo da covariância das variáveis. Dessa forma, a melhor opção para problemas polinomiais é a utilização do $R^2$ ajustado, definido pela fórmula.\n",
    "\n",
    "$$ Adj R^2 = 1 - (1 - R^2) \\frac{n - 1}{n - p - 1} $$\n",
    "<center>onde, $n = tamanho\\ da\\ amostra$ e $p = número\\ de\\ regressores$  </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(240000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 240 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "%matplotlib inline\n",
    "%autosave 240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foram importados 60 arquivos de teste.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "files = [f for f in os.listdir('BlogFeedback/') if re.search(r'blogData_test-.*\\.csv$', f)]\n",
    "test_datasets = []\n",
    "\n",
    "for f in files:\n",
    "    new_test_dataset = pd.read_csv('BlogFeedback/' + f, header=None)\n",
    "    test_datasets.append(new_test_dataset)\n",
    "\n",
    "print('Foram importados %d arquivos de teste.' % len(test_datasets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defina aqui todas as funções de pré processamento, caso necessite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SVR_all_columns_preprocess(testdataset):\n",
    "    print('pre processing', testdataset.shape)\n",
    "    return testdataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando modelos\n",
    "Nesse passo criamos um dicionário com todos os modelos carregados, com suas respectivas funções de pre-processamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Input here your models with columns value (obrigatory)\n",
    "    and pre process function (optional). The latter you have\n",
    "    to define beforehand if using lambda functions and it must\n",
    "    expect a DataFrame of pandas as input.\n",
    "    In models_pathnames list you have to input the path to your\n",
    "    model, in the same sequence that your dictionary is \n",
    "'''\n",
    "models = {\n",
    "    }'Random Forest':{\n",
    "        'columns':[i for i in range(280)]\n",
    "    }\n",
    "}\n",
    "models_pathnames = ['SVR_BOW_blogFeedback.sav', 'KNN_BOW-Zeros_blogFeedback.sav']\n",
    "for index, model in enumerate(models):\n",
    "    try:\n",
    "        models[model]['model'] = joblib.load(models_pathnames[index])\n",
    "    except Exception as e:\n",
    "        print('Error in', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR-All-Columns\n",
      "pre processing (115, 281)\n",
      "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='auto',\n",
      "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=True) \n",
      "\n",
      "KNN_BOW\n",
      "No pre-processing\n",
      "KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "          metric_params=None, n_jobs=-1, n_neighbors=20, p=2,\n",
      "          weights='uniform') \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing if models where loaded correctly\n",
    "for model in models:\n",
    "    print(model)\n",
    "    if 'preprocess' in models[model]:\n",
    "        models[model]['preprocess'](test_datasets[0])\n",
    "    else:\n",
    "        print('No pre-processing')\n",
    "    print(models[model]['model'],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando modelos carregados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Métodos de avaliação\n",
    "##### _Hits_@10\n",
    "For each day of the test data we consider 10 blog pages that were predicted\n",
    "to have to largest number of feedbacks. We count how many out of these pages\n",
    "are among the 10 pages that received the largest number of feedbacks in the\n",
    "reality. We call this evaluation measure Hits@10 and we average Hits@10 for\n",
    "all the days of the test data.\n",
    "##### _AUC_@10\n",
    "For the AUC, i.e., area under the receiver-operator curve, see (Tan et al.,\n",
    "2006), we considered as positive the 10 blog pages receiving the highest number\n",
    "of feedbacks in the reality. Then, we ranked the pages according to their\n",
    "predicted number of feedbacks and calculated AUC. We call this evaluation\n",
    "measure AUC@10.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def hits_10_score(test_target, pred_target):\n",
    "    if not isinstance(test_target, pd.DataFrame):\n",
    "        try:\n",
    "            test_target = pd.DataFrame(test_target.copy())\n",
    "            # test_target = sorted(test_target, reverse=True)\n",
    "        except:\n",
    "            raise Exception('Type {} of test_target not known'.format(type(test_target)))\n",
    "    \n",
    "    if isinstance(pred_target, np.ndarray):\n",
    "        pred_target = pd.DataFrame(pred_target.copy())\n",
    "    elif isinstance(pred_target, pd.DataFrame):\n",
    "        pred_target = pred_target.copy()\n",
    "    else:\n",
    "        raise Exception('Type {} of pred_target not known'.format(type(pred_target)))\n",
    "        \n",
    "    test_target = test_target.sort_values(by=280, axis=0, ascending=False, kind='heapsort')\n",
    "    pred_target = pred_target.sort_values(by=0, axis=0, ascending=False, kind='heapsort')\n",
    "    \n",
    "    top_10_test = test_target.index.values[0:10]\n",
    "    pred_ind = pred_target.index.values\n",
    "    hits = 0\n",
    "    for i in range(10):\n",
    "        hits += 1 if pred_ind[i] in top_10_test else 0\n",
    "        \n",
    "    return hits\n",
    "\n",
    "def auc_10_score(test_target, pred_target):\n",
    "    if not isinstance(test_target, pd.DataFrame):\n",
    "        test_target = pd.DataFrame(test_target.copy())\n",
    "        \n",
    "    if not isinstance(pred_target, pd.DataFrame):\n",
    "        pred_target = pd.DataFrame(pred_target.copy())\n",
    "    \n",
    "    # Getting only the top 10 blog pages index\n",
    "    top10_target_ind = test_target.sort_values(by=280, axis=0, ascending=False, kind='heapsort')\n",
    "    top10_target_ind = top10_target_ind.index.values\n",
    "    top10_target_ind = top10_target_ind[0:10]\n",
    "    \n",
    "    top10_pred_ind = pred_target.sort_values(by=0, axis=0, ascending=False, kind='heapsort')\n",
    "    top10_pred_ind = top10_pred_ind.index.values\n",
    "    top10_pred_ind = top10_pred_ind[0:10]\n",
    "    \n",
    "    # Scaling the prediction to range (0, 1)\n",
    "    mms = MinMaxScaler()\n",
    "    pred_target = mms.fit_transform(pred_target)\n",
    "    \n",
    "    # Transforming test_target in a binary array \"Received many feedback or not\"\n",
    "    zeros = np.zeros(test_target.shape)\n",
    "    test_target = zeros\n",
    "    \n",
    "    # Inputing ones to top 10 blog pages\n",
    "    for i in range(10):\n",
    "        test_target[top10_target_ind[i]] = 1\n",
    "        pred_target[top10_pred_ind[i]] = 1\n",
    "    \n",
    "    score = roc_auc_score(test_target, pred_target)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mSVR-All-Columns\u001b[0m\n",
      "\u001b[95m\t Executing 0 of 60...\u001b[0m\n",
      "pre processing (115, 281)\n",
      "\u001b[95m\t Executing 1 of 60...\u001b[0m\n",
      "pre processing (133, 281)\n",
      "\u001b[95m\t Executing 2 of 60...\u001b[0m\n",
      "pre processing (116, 281)\n",
      "\u001b[95m\t Executing 3 of 60...\u001b[0m\n",
      "pre processing (103, 281)\n",
      "\u001b[95m\t Executing 4 of 60...\u001b[0m\n",
      "pre processing (92, 281)\n",
      "\u001b[95m\t Executing 5 of 60...\u001b[0m\n",
      "pre processing (83, 281)\n",
      "\u001b[95m\t Executing 6 of 60...\u001b[0m\n",
      "pre processing (135, 281)\n",
      "\u001b[95m\t Executing 7 of 60...\u001b[0m\n",
      "pre processing (155, 281)\n",
      "\u001b[95m\t Executing 8 of 60...\u001b[0m\n",
      "pre processing (181, 281)\n",
      "\u001b[95m\t Executing 9 of 60...\u001b[0m\n",
      "pre processing (141, 281)\n",
      "\u001b[95m\t Executing 10 of 60...\u001b[0m\n",
      "pre processing (128, 281)\n",
      "\u001b[95m\t Executing 11 of 60...\u001b[0m\n",
      "pre processing (101, 281)\n",
      "\u001b[95m\t Executing 12 of 60...\u001b[0m\n",
      "pre processing (90, 281)\n",
      "\u001b[95m\t Executing 13 of 60...\u001b[0m\n",
      "pre processing (102, 281)\n",
      "\u001b[95m\t Executing 14 of 60...\u001b[0m\n",
      "pre processing (128, 281)\n",
      "\u001b[95m\t Executing 15 of 60...\u001b[0m\n",
      "pre processing (143, 281)\n",
      "\u001b[95m\t Executing 16 of 60...\u001b[0m\n",
      "pre processing (145, 281)\n",
      "\u001b[95m\t Executing 17 of 60...\u001b[0m\n",
      "pre processing (129, 281)\n",
      "\u001b[95m\t Executing 18 of 60...\u001b[0m\n",
      "pre processing (112, 281)\n",
      "\u001b[95m\t Executing 19 of 60...\u001b[0m\n",
      "pre processing (92, 281)\n",
      "\u001b[95m\t Executing 20 of 60...\u001b[0m\n",
      "pre processing (93, 281)\n",
      "\u001b[95m\t Executing 21 of 60...\u001b[0m\n",
      "pre processing (111, 281)\n",
      "\u001b[95m\t Executing 22 of 60...\u001b[0m\n",
      "pre processing (127, 281)\n",
      "\u001b[95m\t Executing 23 of 60...\u001b[0m\n",
      "pre processing (131, 281)\n",
      "\u001b[95m\t Executing 24 of 60...\u001b[0m\n",
      "pre processing (153, 281)\n",
      "\u001b[95m\t Executing 25 of 60...\u001b[0m\n",
      "pre processing (140, 281)\n",
      "\u001b[95m\t Executing 26 of 60...\u001b[0m\n",
      "pre processing (119, 281)\n",
      "\u001b[95m\t Executing 27 of 60...\u001b[0m\n",
      "pre processing (95, 281)\n",
      "\u001b[95m\t Executing 28 of 60...\u001b[0m\n",
      "pre processing (110, 281)\n",
      "\u001b[95m\t Executing 29 of 60...\u001b[0m\n",
      "pre processing (138, 281)\n",
      "\u001b[95m\t Executing 30 of 60...\u001b[0m\n",
      "pre processing (140, 281)\n",
      "\u001b[95m\t Executing 31 of 60...\u001b[0m\n",
      "pre processing (132, 281)\n",
      "\u001b[95m\t Executing 32 of 60...\u001b[0m\n",
      "pre processing (104, 281)\n",
      "\u001b[95m\t Executing 33 of 60...\u001b[0m\n",
      "pre processing (81, 281)\n",
      "\u001b[95m\t Executing 34 of 60...\u001b[0m\n",
      "pre processing (88, 281)\n",
      "\u001b[95m\t Executing 35 of 60...\u001b[0m\n",
      "pre processing (106, 281)\n",
      "\u001b[95m\t Executing 36 of 60...\u001b[0m\n",
      "pre processing (119, 281)\n",
      "\u001b[95m\t Executing 37 of 60...\u001b[0m\n",
      "pre processing (126, 281)\n",
      "\u001b[95m\t Executing 38 of 60...\u001b[0m\n",
      "pre processing (122, 281)\n",
      "\u001b[95m\t Executing 39 of 60...\u001b[0m\n",
      "pre processing (108, 281)\n",
      "\u001b[95m\t Executing 40 of 60...\u001b[0m\n",
      "pre processing (86, 281)\n",
      "\u001b[95m\t Executing 41 of 60...\u001b[0m\n",
      "pre processing (97, 281)\n",
      "\u001b[95m\t Executing 42 of 60...\u001b[0m\n",
      "pre processing (120, 281)\n",
      "\u001b[95m\t Executing 43 of 60...\u001b[0m\n",
      "pre processing (139, 281)\n",
      "\u001b[95m\t Executing 44 of 60...\u001b[0m\n",
      "pre processing (141, 281)\n",
      "\u001b[95m\t Executing 45 of 60...\u001b[0m\n",
      "pre processing (134, 281)\n",
      "\u001b[95m\t Executing 46 of 60...\u001b[0m\n",
      "pre processing (119, 281)\n",
      "\u001b[95m\t Executing 47 of 60...\u001b[0m\n",
      "pre processing (107, 281)\n",
      "\u001b[95m\t Executing 48 of 60...\u001b[0m\n",
      "pre processing (185, 281)\n",
      "\u001b[95m\t Executing 49 of 60...\u001b[0m\n",
      "pre processing (202, 281)\n",
      "\u001b[95m\t Executing 50 of 60...\u001b[0m\n",
      "pre processing (206, 281)\n",
      "\u001b[95m\t Executing 51 of 60...\u001b[0m\n",
      "pre processing (134, 281)\n",
      "\u001b[95m\t Executing 52 of 60...\u001b[0m\n",
      "pre processing (133, 281)\n",
      "\u001b[95m\t Executing 53 of 60...\u001b[0m\n",
      "pre processing (126, 281)\n",
      "\u001b[95m\t Executing 54 of 60...\u001b[0m\n",
      "pre processing (106, 281)\n",
      "\u001b[95m\t Executing 55 of 60...\u001b[0m\n",
      "pre processing (110, 281)\n",
      "\u001b[95m\t Executing 56 of 60...\u001b[0m\n",
      "pre processing (132, 281)\n",
      "\u001b[95m\t Executing 57 of 60...\u001b[0m\n",
      "pre processing (175, 281)\n",
      "\u001b[95m\t Executing 58 of 60...\u001b[0m\n",
      "pre processing (191, 281)\n",
      "\u001b[95m\t Executing 59 of 60...\u001b[0m\n",
      "pre processing (214, 281)\n",
      "\u001b[94mExecution time for SVR-All-Columns: 94.24177622795105\u001b[0m\n",
      "\u001b[95mKNN_BOW\u001b[0m\n",
      "\u001b[95m\t Executing 0 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 1 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 2 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 3 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 4 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 5 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 6 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 7 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 8 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 9 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 10 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 11 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 12 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 13 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 14 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 15 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 16 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 17 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 18 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 19 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 20 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 21 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 22 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 23 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 24 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 25 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 26 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 27 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 28 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 29 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 30 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 31 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 32 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 33 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 34 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 35 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 36 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 37 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 38 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 39 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 40 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 41 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 42 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 43 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 44 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 45 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 46 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 47 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 48 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 49 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 50 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 51 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 52 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 53 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 54 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 55 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 56 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 57 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 58 of 60...\u001b[0m\n",
      "\u001b[95m\t Executing 59 of 60...\u001b[0m\n",
      "\u001b[94mExecution time for KNN_BOW: 6.779041051864624\u001b[0m\n",
      "\u001b[92mSaved results to 'results.json'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "{\n",
    "    'model_name': {\n",
    "        'hits_10' = [1, 2, ...],\n",
    "        'auc_10' = [.1, .3, ...],\n",
    "        'r_sqr' = [.3, .4, ...],\n",
    "        't_test' = [10, 40, ...]\n",
    "    }\n",
    "}\n",
    "'''\n",
    "results = {}\n",
    "for model in models:\n",
    "    \n",
    "    # Creating Lists\n",
    "    results[model] = {}\n",
    "    results[model]['hits_10'] = []\n",
    "    results[model]['auc_10'] = []\n",
    "    results[model]['r_sqr'] = []\n",
    "    results[model]['t_test'] = []\n",
    "    print(bcolors.HEADER +\n",
    "          model\n",
    "         + bcolors.ENDC)\n",
    "\n",
    "    t_all = time()\n",
    "    for i, test_dataset in enumerate(test_datasets):\n",
    "        print(bcolors.HEADER +\n",
    "              '\\t Executing {} of {}...'.format(i, len(test_datasets))\n",
    "             + bcolors.ENDC)\n",
    "        # Pre process data, if necessary\n",
    "        dtest = None\n",
    "        if 'preprocess' in models[model]:\n",
    "            dtest = models[model]['preprocess'](test_dataset)\n",
    "        else:\n",
    "            dtest = test_dataset\n",
    "        \n",
    "        # Selecting columns to model and separating target\n",
    "        y_test = dtest.loc[:, [280]]\n",
    "        columns = models[model]['columns']\n",
    "        dtest = dtest.loc[:, columns]\n",
    "        \n",
    "        t_test = time()\n",
    "\n",
    "        # Predict values\n",
    "        try:\n",
    "            y_pred = models[model]['model'].predict(dtest)\n",
    "        except Exception as e:\n",
    "            print(bcolors.FAIL + \n",
    "                  'Error in predict in model {} in dataset {} with error \\'{}\\''\n",
    "                      .format(model, files[i], e) + \n",
    "                  bcolors.ENDC)\n",
    "            continue\n",
    "\n",
    "        # Evaluate model\n",
    "        try:\n",
    "            r_sqr = r2_score(y_test, y_pred)\n",
    "        except Exception as e:\n",
    "            r_sqr = np.NaN\n",
    "            print(bcolors.FAIL + \n",
    "                  'Error in r^2 in file {}, with message \\'{}\\''\n",
    "                      .format(files[i], e) + \n",
    "                  bcolors.ENDC)\n",
    "            \n",
    "        try:\n",
    "            hits_10 = hits_10_score(y_test, y_pred)\n",
    "        except Exception as e:\n",
    "            hits_10 = np.NaN\n",
    "            print(bcolors.FAIL + \n",
    "                  'Error in hits_10 in file {}, with message \\'{}\\''\n",
    "                      .format(files[i], e) + \n",
    "                  bcolors.ENDC)\n",
    "        \n",
    "        try:\n",
    "            auc_10 = auc_10_score(y_test, y_pred)\n",
    "        except Exception as e:\n",
    "            auc_10 = np.NaN\n",
    "            print(bcolors.FAIL + \n",
    "                  'Error in auc_10 in file {}, with message \\'{}\\''\n",
    "                      .format(files[i], e) + \n",
    "                  bcolors.ENDC)\n",
    "        \n",
    "        t_test = time() - t_test\n",
    "        results[model]['hits_10'].append(hits_10)\n",
    "        results[model]['auc_10'].append(auc_10)\n",
    "        results[model]['r_sqr'].append(r_sqr)\n",
    "        results[model]['t_test'].append(t_test)\n",
    "        \n",
    "    print(bcolors.OKBLUE + \n",
    "          'Execution time for {}: {}'\n",
    "              .format(model, time() - t_all) + \n",
    "          bcolors.ENDC)\n",
    "        \n",
    "\n",
    "import json\n",
    "with open('results.json', 'w') as json_file:\n",
    "    json.dump(results, json_file)\n",
    "    print(bcolors.OKGREEN + \n",
    "           'Saved results to \\'results.json\\'' \n",
    "           + bcolors.ENDC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mSVR-All-Columns\u001b[0m\n",
      "\u001b[94m\thits_10: 0.6833333333333333 +- 0.7163888888888889\u001b[0m\n",
      "\u001b[94m\tauc_10: 0.5129493842485033 +- 0.00012061908656415746\u001b[0m\n",
      "\u001b[94m\tr_sqr: -0.029942005684399333 +- 0.0001744760111074845\u001b[0m\n",
      "\u001b[94m\tt_test: 1.5680464029312133 +- 0.16904839933229387\u001b[0m\n",
      "\u001b[95mKNN_BOW\u001b[0m\n",
      "\u001b[94m\thits_10: 4.466666666666667 +- 1.8822222222222222\u001b[0m\n",
      "\u001b[94m\tauc_10: 0.8526940745909867 +- 0.005407660440054797\u001b[0m\n",
      "\u001b[94m\tr_sqr: 0.14013292969036856 +- 0.4425672596924402\u001b[0m\n",
      "\u001b[94m\tt_test: 0.11015123128890991 +- 7.74612186399774e-06\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for model in results:\n",
    "    print(bcolors.HEADER +\n",
    "          model\n",
    "         + bcolors.ENDC)\n",
    "    for key in results[model]:\n",
    "        result = np.array(results[model][key])\n",
    "        print(bcolors.OKBLUE +\n",
    "              '\\t{}: {} +- {}'.format(key, np.mean(result), result.var())\n",
    "             + bcolors.ENDC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
